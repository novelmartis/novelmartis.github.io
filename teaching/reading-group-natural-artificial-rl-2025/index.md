---
layout: page
title: Reading Group on Natural and Artificial Reinforcement Learning (2025)
permalink: /teaching/reading-group-natural-artificial-rl-2025/
---

# Reading Group on Natural and Artificial Reinforcement Learning

<p class="muted">
Reading group course (2025) where each session centered on a seminal paper in reinforcement learning, spanning animal learning, cognitive neuroscience, and modern machine learning.
</p>

<hr class="hr">

<div class="projlist">

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1037/0003-066X.43.3.151">Pavlovian conditioning: It's not what you think it is (Rescorla, 1988)</a>
      </div>
    </div>
    <p class="pdesc">
      Associative learning foundations: showed that conditioning goes beyond simple stimulus-response pairings, emphasizing richer statistical and structural relationships in learning.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1037/0003-066X.43.3.151">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1007/BF00992696">Simple statistical gradient-following algorithms for connectionist reinforcement learning (Williams, 1992)</a>
      </div>
    </div>
    <p class="pdesc">
      Policy-gradient foundations (REINFORCE): introduced unbiased gradient estimators for policy optimization and established a core model-free paradigm still used today.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1007/BF00992696">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1145/203330.203343">Temporal difference learning and TD-Gammon (Tesauro, 1995)</a>
      </div>
    </div>
    <p class="pdesc">
      Temporal-difference milestone: an early high-impact value-based RL success, demonstrating that self-play and TD updates could reach expert-level behavior in complex games.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1145/203330.203343">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1162/089976606775093909">Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia (O'Reilly and Frank, 2006)</a>
      </div>
    </div>
    <p class="pdesc">
      Cortico-BG-PFC loops in human RL: a biologically grounded actor-critic account of working-memory gating and control, bridging neural circuits with computational RL.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1162/089976606775093909">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1016/j.cognition.2009.07.005">Action understanding as inverse planning (Baker, Saxe, and Tenenbaum, 2009)</a>
      </div>
    </div>
    <p class="pdesc">
      Hierarchical and neurosymbolic RL in humans: framed social cognition as Bayesian inverse planning, inferring latent goals from observed actions.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1016/j.cognition.2009.07.005">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1038/nature14236">Human-level control through deep reinforcement learning (Mnih et al., 2015)</a>
      </div>
    </div>
    <p class="pdesc">
      DQN revolution: combined deep neural networks with Q-learning and replay/target stabilization to learn strong Atari control directly from pixels.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1038/nature14236">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1609/aaai.v31i1.10916">The Option-Critic Architecture (Bacon, Harb, and Precup, 2017)</a>
      </div>
    </div>
    <p class="pdesc">
      Hierarchical RL with temporal abstraction: learned options end-to-end, including intra-option policies and termination, making hierarchy part of optimization.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1609/aaai.v31i1.10916">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1038/s41593-018-0232-z">Prioritized memory access explains planning and hippocampal replay (Mattar and Daw, 2018)</a>
      </div>
    </div>
    <p class="pdesc">
      Hippocampal replay and model-based RL: proposed a normative replay-prioritization rule linking planning utility to forward/backward replay patterns observed in biology.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1038/s41593-018-0232-z">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://arxiv.org/abs/2010.02193">Mastering Atari with discrete world models (Hafner et al., 2021)</a>
      </div>
    </div>
    <p class="pdesc">
      Advanced model-based RL (DreamerV2): learned a world model and optimized behavior in imagined trajectories, showing strong Atari performance with latent-space planning.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://arxiv.org/abs/2010.02193">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://doi.org/10.1016/j.neuron.2021.09.034">The learning of prospective and retrospective cognitive maps within neural circuits (Namboodiri and Stuber, 2021)</a>
      </div>
    </div>
    <p class="pdesc">
      Prospective and retrospective RL in brain circuits: highlighted successor and predecessor style representations to explain forward- and backward-looking credit assignment.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://doi.org/10.1016/j.neuron.2021.09.034">Paper</a>
    </div>
  </div>

  <div class="pcard compact">
    <div class="phead">
      <div class="ptitle">
        <a href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Shao et al., 2024)</a>
      </div>
    </div>
    <p class="pdesc">
      RL in large language models (GRPO): showed how group-relative policy optimization can improve mathematical reasoning while keeping training efficient.
    </p>
    <div class="plinks">
      <a class="btn primary" href="https://arxiv.org/abs/2402.03300">Paper</a>
    </div>
  </div>

</div>
