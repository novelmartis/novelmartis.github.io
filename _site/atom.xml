<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>martis.</title>
 <link href="http://novelmartis.github.io/atom.xml" rel="self"/>
 <link href="http://novelmartis.github.io/"/>
 <updated>2016-07-14T01:35:55+02:00</updated>
 <id>http://novelmartis.github.io</id>
 <author>
   <name>‘Sushrut Thorat’</name>
   <email></email>
 </author>

 
 <entry>
   <title>Let's get it started!</title>
   <link href="http://novelmartis.github.io/2016/07/12/getting-started/"/>
   <updated>2016-07-12T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2016/07/12/getting-started</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The stage is set. Time to roll..&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Only if life were so. The stage is seldom &lt;em&gt;set&lt;/em&gt;. One should keep rolling though.. or atleast think so. Too philosophical a start!&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll pull in some of my past blog posts, which are scattered all over the place. Then, hopefully, I will keep up the momentum. x&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll eventually add in the project details too.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Arithmetic computations with Spiking Neural Networks</title>
   <link href="http://novelmartis.github.io/2016/06/09/arithmetic-computation/"/>
   <updated>2016-06-09T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2016/06/09/arithmetic-computation</id>
   <content type="html">&lt;p&gt;&lt;em&gt;This one dates back to 2015. It was a part of my Bachelor’s thesis. But this part has applications beyond the objective of the thesis viz. quadcopter control. We published a paper detailing our method. It is available &lt;a href=&quot;http://dx.doi.org/10.1109/IJCNN.2015.7280822&quot;&gt;here&lt;/a&gt; (&lt;a href=&quot;https://www.academia.edu/20315873/Arithmetic_Computing_via_Rate_Coding_in_Neural_Circuits_with_Spike-triggered_Adaptive_Synapses&quot;&gt;open-access&lt;/a&gt;). Note: The description below is a simplistic version of what was done in the paper. All the operations mentioned use well-defined mathematical rules.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spiking_neural_network&quot;&gt;Spiking Neural Networks&lt;/a&gt; are approximations of &lt;a href=&quot;https://en.wikipedia.org/wiki/Biological_neural_network&quot;&gt;biological neural networks&lt;/a&gt;. The approximations range from neuron models to synaptic learning models to network topology. The &lt;a href=&quot;http://eaton.math.rpi.edu/CSUMS/Papers/Neuro/Izhikevich04.pdf&quot;&gt;simplest spiking neuron models&lt;/a&gt; are the LIF, AEIF, and Izhikevich neurons. They try to capture information about the spike-times, approximate the nature of the action potential, and various operational modes (burst, regular, etc.). An extremely realistic model is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model&quot;&gt;Hodgkin-Huxley model&lt;/a&gt;. But it is computationally-intensive, and without access to clusters you cannot simulate a network with hundreds of such neurons. Processes such as &lt;a href=&quot;http://research.mssm.edu/cnic/pdfs/nn0604-567.pdf&quot;&gt;dendritic integration&lt;/a&gt; and AP back-propagation are unaccounted for in these models. But we have to start somewhere, and we usually start simple. AEIF neurons capture many neural firing modes, and other relevant properties such as &lt;a href=&quot;http://www.bio.lmu.de/%7Ebenda/publications/adaptation03/adaptationh.html&quot;&gt;spike-frequency adaptation&lt;/a&gt; and also encode a refractory function. These are the neurons we use in our method.&lt;/p&gt;

&lt;p&gt;So, the problem was this - how can SNNs implement arithmetic operations such as Addition, Subtraction, Multiplication and Division? This turns out to be a non-trivial question. Spiking neurons have non-linear I-O relationships (logarithmic &lt;a href=&quot;ftp://ftp.icsi.berkeley.edu/pub/ai/jagota/vol2_6.pdf&quot;&gt;transfer function&lt;/a&gt;), as shown in the following figure. &lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://novelmartis.github.io/assets/fI_AEIF.png&quot; alt=&quot;f-I plot for AEIF neuron&quot;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;To implement linear operations such as addition or subtraction, we need to linearise the the transfer function. Now, we focussed on the network influences, rather than trying to &lt;a href=&quot;http://arxiv.org/pdf/1410.7881.pdf&quot;&gt;manipulate internal dynamics of neurons&lt;/a&gt;. To that end, we introduced a self-inhibitory loop that pushed the neuron into the higher current domain in the above figure, which is pretty linear. With a linear transfer function at our disposal, we can do addition and subtraction. &lt;/p&gt;

&lt;p&gt;The problem begins with multiplication. I thought about using a recurrent loop with some sort of gated function which would keep adding the same value till we ask to stop it. But then I realised there was a simpler way - use exponentiation and logarithm. &lt;/p&gt;

&lt;p&gt;Now, $$\text{A}\times\text{B}=e^{\text{log(A)} + \text{log(B)}}$$&lt;/p&gt;

&lt;p&gt;So, if we could construct networks with overall exponential and logarithmic transfer functions, we are done! We not only solve multiplication and division, but we can also create any sort of polynomial transfer function by composing appropriate LOG and EXP blocks!&lt;/p&gt;

&lt;p&gt;To implement these LOG and EXP blocks, we turned to adaptive synapses. We designed learning rules that allowed use to generate LOG and EXP responses. Simply put, to generate LOG, the synaptic weight has to decrease with increased pre-synaptic firing rate, and to generate EXP, it has increase with increasing pre-synaptic firing rate. We then translated these firing rate based rules into spike-time based rules for performing real-time computations. So, we had EXP and LOG networks we which composed to implement multiplication and division.&lt;/p&gt;

&lt;p&gt;Yes, we know how to use SNNs to implement addition, subtraction, multiplication, and division now. But there were multiple problems with the approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The synaptic adaptation rule was designed and doesn’t seem to be biologically realistic.&lt;/li&gt;
&lt;li&gt;The time required for the networks to stabilise their multiplicative outputs could extend to 0.5 seconds, which make them impossible to use in quick-response control systems such as the quadcopter core control, or biological systems like the cerebellum.&lt;/li&gt;
&lt;li&gt;The operational spike rates lie between 40-140 Hz, which excludes most of the biological neural rates which are pretty low.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, this is not a biologically realistic model of arithmetic computations, and shouldn’t be treated as such.&lt;/p&gt;

&lt;p&gt;A more realistic method of implementing arithmetic operations needs to take into account the actual operational principles of biological neural networks such as &lt;a href=&quot;http://www.nature.com/neuro/journal/v19/n3/full/nn.4243.html&quot;&gt;tight E-I balance&lt;/a&gt;. There are some &lt;a href=&quot;https://papers.nips.cc/paper/5948-enforcing-balance-allows-local-supervised-learning-in-spiking-recurrent-networks.pdf&quot;&gt;efforts&lt;/a&gt; in this direction. Hopefully, we’ll get to a point where we would be able to converse fluently in neural codes.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Quadcopter control using Spiking Neural Networks</title>
   <link href="http://novelmartis.github.io/2016/06/05/quadcopter-control-using-snn/"/>
   <updated>2016-06-05T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2016/06/05/quadcopter-control-using-snn</id>
   <content type="html">&lt;p&gt;&lt;em&gt;This one dates back to 2015. This was the topic of my bachelor’s thesis, which was admittedly left incomplete. Find the report &lt;a href=&quot;https://dx.doi.org/10.6084/m9.figshare.1582657.v1&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spiking_neural_network&quot;&gt;Spiking Neural Networks&lt;/a&gt; hit that sweet spot between biological neural networks and artificial neural networks (although now &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;RNNs&lt;/a&gt; and &lt;a href=&quot;http://arxiv.org/abs/1605.07678&quot;&gt;CNNs&lt;/a&gt; are struggling to take over). To better understand how they function, we thought about using them to solve a well-known complex control problem - balancing and flying a quadcopter. &lt;/p&gt;

&lt;p&gt;The first step was to understand quadcopter flight dynamics. The quadcopter is inherently an unstable system - the smallest uncorrected angular deviation in pitch can send the quadcopter flying out of control. PIDs (compensatory mechanisms that worked by controlling the four rotors) were simple and efficient solutions to this problem. In the first leg of the project, we designed a control algorithm directly taking the dynamical equations of the system into account. Our algorithm outperformed &lt;a href=&quot;https://en.wikipedia.org/wiki/PID_controller&quot;&gt;PID&lt;/a&gt;s, and could recover the quadcopter from a multitude of states (including crazy upside-down states). &lt;/p&gt;

&lt;p&gt;Having understood quadcopter dynamics, we wanted to move ahead to bring in the SNNs. The roadblock was severe. How were we supposed to use SNNs to fly a quadcopter? There were multiple options -&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Learning algorithms such as &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.6325&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;ReSuMe&lt;/a&gt;, &lt;/li&gt;
&lt;li&gt;Evolutionary methods such as &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.5457&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;NEAT&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Hand-assembling a network,&lt;/li&gt;
&lt;li&gt;Understanding a natural control system such as the cerebellum and trying to re-purpose the mechanisms. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(4) is a hard problem, but would definitely be interesting to address someday. We weren’t aware if somebody had used (1), but I wasn’t really impressed by the method. Also, the training would require data from the control system we developed, and we were not sure if the the algorithm would generalise and ‘go beyond’ - which would be the point of using a learning algorithm. (But maybe the ‘i-dont-like-it&amp;#39; factor weighed heavier) (2) was used in a &lt;a href=&quot;http://classes.engr.oregonstate.edu/mime/fall2010/me537/Papers/NN_EA_application_shepherd.pdf&quot;&gt;paper&lt;/a&gt; successfully, but not with SNNs (NEAT with SNNs is a computationally-heavy routine).&lt;/p&gt;

&lt;p&gt;So, were we supposed to hand-assemble the network? That sounded sour. But then my advisor hinted at building plug-and-play spiking network modules which could be assembled to emulate any control algorithm. We were focussed on the basic arithmetic operations - addition, subtraction, multiplication and division - which mostly are sufficient for implementing simple control systems. Also, the control algorithm we designed in the first leg of the project used only polynomial operations. So, we set out on this path, and we came up with a method which lets one implement any polynomial transform on real-time spike trains, and &lt;a href=&quot;http://dx.doi.org/10.1109/IJCNN.2015.7280822&quot;&gt;published a paper&lt;/a&gt; about it. (I will discuss the details in another post)&lt;/p&gt;

&lt;p&gt;It so turned out that the time lags involved in the more complex operations such as multiplication of variables raised to some power, were huge. It became extremely hard to combine the modules we designed to emulate the control algorithm we had built. By that time, my time at IIT Bombay was up, and the project was left unfinished. As tempting as the approach sounded, I now think that it wasn’t the best way to proceed.&lt;/p&gt;

&lt;p&gt;It would be interesting to get around the time lag problem, but I’d put my bets on another approach. In any case, it was an awesome experience trying to bend SNNs to our will - seems we lost.&lt;/p&gt;
</content>
 </entry>
 

</feed>
