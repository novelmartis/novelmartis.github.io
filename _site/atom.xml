<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>martis.</title>
 <link href="http://novelmartis.github.io/atom.xml" rel="self"/>
 <link href="http://novelmartis.github.io/"/>
 <updated>2017-10-29T18:46:42+01:00</updated>
 <id>http://novelmartis.github.io</id>
 <author>
   <name>‘Sushrut Thorat’</name>
   <email></email>
 </author>

 
 <entry>
   <title>Notes about the Donders Discussions 2017</title>
   <link href="http://novelmartis.github.io/2017/10/29/donders-discussions/"/>
   <updated>2017-10-29T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2017/10/29/donders-discussions</id>
   <content type="html">&lt;p&gt;Donders Discussions is a neuroscience conference organised by doctoral researchers from the Donders Institute. This is the first neuroscience conference I attended. The venue - De Hamel, Nijmegen - is a perfect location for such conferences given it&amp;#39;s carefree atmosphere and proximity to a beautiful landscape. It was enriching to sit through sessions about the problems in neuroscience which I do not deal with on a regular basis. I guess it will take time for that information to register in a useful context.&lt;/p&gt;

&lt;p&gt;There were 16 sessions (4 sessions running parallely in a group, 4 groups), 4 keynote speeches, and 2 poster sessions over the two days of the conference. I attended some of them and am writing about the interesting bits here.&lt;/p&gt;

&lt;h2&gt;Keynote speeches&lt;/h2&gt;

&lt;h4&gt;&lt;a href=&quot;https://scholar.google.nl/citations?user=Z3UgRXsAAAAJ&amp;amp;hl=en&amp;amp;oi=sra&quot;&gt;Nael Nadif Kasri&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;He talked about developing therapy for brain disorders. I am sure this a big sub-field of neuroscience. The best part was about developing neural cultures with particular gene knockouts mirroring the knockouts in particular disorders. The synchronisation of the resulting spiking activity was analysed and they could find distinct activity clusters for the different disorders, as seen in his slide below.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://novelmartis.github.io/assets/dd_nnk.JPG&quot; alt=&quot;Clustering of different genetic disorders&quot;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;He suggested that by generating targeted therapies for normalising the activity patterns in these cultures, we could develop therapies that could transfer better to humans than from research on mice bred with similar knockouts. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It would be informative to check if randomly connected spiking neural networks could be manipulated to generate those different spike patterns, in effect pinpointing the neural dynamics or network connectivity parameters that could used to normalise the activity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;&lt;a href=&quot;http://www.ru.nl/english/people/slors-m/&quot;&gt;Marc Slors&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;He&amp;#39;s a philosopher who deals with philosophical questions regarding consciousness, free will and the relationship between cognition and culture. He started out by describing the historical shift in perspective about what constitutes cognition (embodied cognition, etc.) It was fun until he claimed that neuroscientists do not account for interactions with the environment while thinking about cognition and its development. That is not true at all. As such, I talked about that with him and it seemed he wasn&amp;#39;t aware of the recent literature which deals with the relationship between cognition and culture (read: effects of language on cognition, etc.) I hope we cannot generalise this to all philosophers. I guess it would have been better if he would&amp;#39;ve focussed on the consciousness and free will parts - parts where I&amp;#39;d say neuroscientists have little or no clue what to do and would appreciate any new perspectives. &lt;/p&gt;

&lt;h2&gt;Parallel sessions&lt;/h2&gt;

&lt;p&gt;There were 4 sessions running parallely. I could attend one for each of the 4 groups.&lt;/p&gt;

&lt;h3&gt;Top-down influences on perception&lt;/h3&gt;

&lt;h4&gt;&lt;a href=&quot;https://scholar.google.nl/citations?user=VKDX28YAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&quot;&gt;Benedikt Ehinger&lt;/a&gt; : Filled-in percepts from the blind spots are judged as more reliable than veridically seen ones&lt;/h4&gt;

&lt;p&gt;This is a counter-intuitive &lt;a href=&quot;https://doi.org/10.7554/eLife.21761.001&quot;&gt;finding&lt;/a&gt;. Subjects were presented with the stimuli shown in the figure below (from his paper), and subjects had to make a forced-choice about which of the two stimuli were more consistent in their orientations.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://novelmartis.github.io/assets/DD_ehinger.png&quot; alt=&quot;Stimuli in the experiment&quot;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Why do we trust the information in the blind spot more in this case? His prominent explanation relies on predictive coding. The blind spot is filled in with information consistent with its surroundings. Now even if the surrounding input becomes noisy, the top-down filling won&amp;#39;t be as noisy, so the information received from the blind spot will always match the prediction, thus assigning more weight towards making a decision. This is also supported by their observation that subjects were faster in selecting the blind spot stimuli as opposed to the case where both stimuli were presented outside the blind spot. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Although the given description might make sense, it is very weird that the brain doesn&amp;#39;t know what it is filling in and what not. I thought this might be an attentional effect. Because the brain might know where the blind spot is, it might have heuristics in place to devote more attentional resources to the area of the blind spot. Benedikt replied that they had performed a saccade experiment with the same stimuli and they found no effect of the blind spot. Yes, spatial attention is shifted before saccades in such cases, but the attentional effect might be independent of eye movements too. This is unclear to me. It nevetheless is an interesting study and I somehow feel such effects might not be reserved exclusively for the blind spot (think inattentional blindness and stuff).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;&lt;a href=&quot;https://scholar.google.nl/citations?user=jSooh8YAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&quot;&gt;Nadine Dijkstra&lt;/a&gt; : Perception as a restricted form of imagery&lt;/h4&gt;

&lt;p&gt;Using Dynamic casual modelling (DCM), she analysed functional connectivity profiles of four regions of the brain during perception and imagery, showing that both bottom-up and top-down information is involved in perception, and more top-down during imagery, as seen in the figure below (taken from her &lt;a href=&quot;https://dx.doi.org/10.1038%2Fs41598-017-05888-8&quot;&gt;paper&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://novelmartis.github.io/assets/dd_nadine.png&quot; alt=&quot;DCM results&quot;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;These results were expected given previous studies. I think it would be cool to model the same using known anatomical connectivity profiles between these regions. I find it very weird that the inferior frontal gyrus would be connected to the early visual cortex via a direct connection, and even if it is aren&amp;#39;t there other pathways which might be carrying the information reflected in the shown DCM analysis?&lt;/p&gt;

&lt;h3&gt;Cognitive maps and abstract spaces: the role of spatial neural mechanisms in other cognitive domains&lt;/h3&gt;

&lt;h4&gt;&lt;a href=&quot;https://www.semanticscholar.org/author/Alexandra-O-Constantinescu/5382823&quot;&gt;Alexandra Constantinescu&lt;/a&gt; : Organizing conceptual knowledge in humans with a gridlike code&lt;/h4&gt;

&lt;p&gt;Now, I know very little about grid and place cells, so this description might be naive. Grid cells are known to be involved in spatial representation and navigation. Alexandra&amp;#39;s &lt;a href=&quot;https://doi.org/10.1126/science.aaf0941&quot;&gt;work&lt;/a&gt; shows that the hexagonal grid code is not just for the spatial encoding of the environment but might be involved in the representation of any conceptual spaces. In the study, they used images of a stork-like bird whose neck and leg size was manipulated. In this two-dimensional conceptual space, they trained subjects to associate six bird shapes (evenly spread on a hexagon) with symbols so as to make them proficient in the differences. During scanning, they were shown videos of one bird morphing into another which could be thought of as moving along a line in the 2D space. What they found was the fMRI responses in the entorhinal cortex and other regions (similar to the case of spatial navigation) peaked when the movement directions in the 2D space were along the lines joining opposite vertices of some hexagon. This observation suggests that the conceptual space is being mapped with a hexagonal grid-like code! &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I somehow feel this result would be stronger if they had used 4 or any other symmetry in their training stimuli.&lt;/li&gt;
&lt;li&gt;When asked why a hexagonal code would be optimal, I was pointed to &lt;a href=&quot;https://doi.org/10.7554/eLife.05979.001&quot;&gt;Mathias et al., eLife 2016&lt;/a&gt;, which alludes to hexagonal close packing being the optimal lattice packing, where lattice packing seems to be compared with packing concepts in a representational space. (I still have to read the paper)&lt;/li&gt;
&lt;li&gt;It would be cool if other such representational spaces could be shown to be mapped by a grid-like code. There was a poster by Simone Viganò (Cortical effects of categorical learning for new words and multisensory objects) which had multisensory stimuli with varying object size and tone pitch, where he tried to answer if assigning categories to these stimuli would change their representation (answer is yes). I still am trying to figure out if the grid formalism could be tested using the fMRI data he already has (obtained from an image 1-back task).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Decision neuroscience&lt;/h3&gt;

&lt;h4&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Chih_Chung_Ting&quot;&gt;Chih-Chung Ting&lt;/a&gt; : Incidental Anxiety enhances reinforcement learning for gains but not for losses&lt;/h4&gt;

&lt;p&gt;There was a weird, yet intuitive, aspect of human behaviour which was presented as a part of his work. Chih-Chung&amp;#39;s study builds upon the paradigm of &lt;a href=&quot;https://doi.org/10.1038/ncomms9096&quot;&gt;Palminteri et al., Nature 2015&lt;/a&gt; (introduces the factor of anxiety), and the relevant results are also seen in that paper. They presented 8 objects to the subjects with different levels of probabilistic punishment or gain, as seen in the figure below (from the paper). Now when two of these stimuli are shown, the subjects had to select one to maximise reward. The partial and complete cases refer to if the subjects were given feedback just about their choice or also about the alternate choice. Depending on these conditions, the subjects show a different response profile after training, as seen in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://novelmartis.github.io/assets/dd_cct.png&quot; alt=&quot;Palminteri et al., 2015 results&quot;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;The response during the &amp;#39;complete&amp;#39; condition is weird - the subjects prefer low punishment over low reward! This observation is replicated in Chih-Chung&amp;#39;s findings. They suggest that we are reinforced to choose the low punishment more than we are reinforced to choose low reward. If one thinks about it, if given sufficient time, a rational agent would choose the low reward anytime. These findings reflect an automatic response it seems. This doesn&amp;#39;t seem to happen in the &amp;#39;partial&amp;#39; condition though. (Have to read the paper carefully - maybe they have an explanation).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Thoughts about the 1st Brain Twitter Conference</title>
   <link href="http://novelmartis.github.io/2017/05/27/twitter-conference/"/>
   <updated>2017-05-27T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2017/05/27/twitter-conference</id>
   <content type="html">&lt;p&gt;&lt;em&gt;I presented &lt;a href=&quot;https://twitter.com/abc_aalto/status/855043613453058048&quot;&gt;my work&lt;/a&gt; at the 1st Brain Twitter Conference, hosted by the Aalto Brain Centre. Here are my thoughts about the experience.&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;Briefing&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://storify.com/abc_aalto/brain-twitter-conference-braintc&quot;&gt;1&lt;sup&gt;st&lt;/sup&gt; Brain Twitter Conference&lt;/a&gt; was hosted on Twitter on the 20th of April, 2017. The conference events consisted of Twitter &amp;quot;talks&amp;quot; (let&amp;#39;s call them Twalks). Each twalk had the structure:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Announcement tweet of the twalk by the &lt;a href=&quot;https://twitter.com/abc_aalto&quot;&gt;Aalto Brain Centre&lt;/a&gt;, who acted as the chair.&lt;/li&gt;
&lt;li&gt;A string of &lt;em&gt;n&lt;/em&gt; tweets by the presenter (with images/gifs/videos attached)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There were two types of twalks - presentations and keynotes. Each presentation consisted of 6 tweets by the presenter, and the keynote consisted of 10 tweets. Each presentation was allotted 15 minutes for the tweet delivery while the keynotes lasted 30 minutes. During the presentation and the keynotes, others were free to ask questions, and all the proceedings were recorded under the hashtag &lt;a href=&quot;https://twitter.com/search?vertical=default&amp;amp;q=%23braintc&amp;amp;src=typd&amp;amp;lang=en&quot;&gt;#brainTC&lt;/a&gt;. Aalto Brain Centre retweeted the tweets of the twalks so one could easily cut through the comments and follow the twalks.&lt;/p&gt;

&lt;p&gt;Ckeck out the &lt;a href=&quot;https://storify.com/abc_aalto/brain-twitter-conference-braintc&quot;&gt;Storify&lt;/a&gt; of the conference.&lt;/p&gt;

&lt;h2&gt;Thoughts about the Conference&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Condensing your work to 6 tweets seems like an impossibility, but the conference showed that it is possible without toning down scientific rigour. Images become extremely important and need to stand their own ground. Using images directly from your publication seems tempting, but the captions seem cumbersome. The focus should be on the pictorial flow of ideas within the image. It wasn&amp;#39;t an easy task for me (took a week and multiple discussions with the team to get things right). It was fun.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://twitter.com/search?vertical=default&amp;amp;q=%23braintc&amp;amp;src=typd&amp;amp;lang=en&quot;&gt;#braintc&lt;/a&gt; was abuzz with discussions. There weren&amp;#39;t many well-known names. Maybe they were merely spectating. It somehow felt like the initial outreach of the conference was not sufficient to capture the attention of well-known researchers. I guess after the success of this edition, the next edition will enjoy a wider group of participants. Given the ease (?) of organising, compared to other conferences, I guess we should have two or three Brain Twitter Conferences in a year. It was awesome of the &lt;a href=&quot;https://twitter.com/abc_aalto&quot;&gt;Aalto Brain Centre&lt;/a&gt; to host the first edition, but if we are to host multiple conferences in a year, other research groups should pick up the torch.&lt;/li&gt;
&lt;li&gt;We had a nice &lt;a href=&quot;https://twitter.com/Neuro_Skeptic/status/855010583166496768&quot;&gt;discussion&lt;/a&gt; about the importance of scientific blogging as a part of PhD training. Most research is highly specific. Viewing the work in a broader perspective would be useful to us as scientists and would make it easier for the public to understand what we are doing. That discussion got me thinking - it wouldn&amp;#39;t be a bad idea to host panel discussions on such topics as a part of the conference. The organiser&amp;#39;s Twitter handle could tweet about the issue/idea, and the discussion could follow as replies to that tweet creating a tree under that tweet. That could be tried in the next edition. We could also have a dedicated event where people provide general introductions to new ideas, which might be half-baked, and could receive feedback about them from the community.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All in all, it was an enjoyable experience. I remember people being skeptical about this format, but it turned out fine. I am looking forward to many more of these conferences!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Building a simple Reverse Dictionary</title>
   <link href="http://novelmartis.github.io/2016/11/06/reverse-dictionary/"/>
   <updated>2016-11-06T00:00:00+01:00</updated>
   <id>http://novelmartis.github.io/2016/11/06/reverse-dictionary</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The paper can be accessed on &lt;a href=&quot;https://arxiv.org/abs/1606.00025&quot;&gt;arXiv&lt;/a&gt;. Setting a new baseline for deep phrasal semantic processing. The test dataset and sample code can be found on &lt;a href=&quot;https://github.com/novelmartis/RD16demo&quot;&gt;Github&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;Motivation &amp;amp; Idea:&lt;/h4&gt;

&lt;p&gt;We often can&amp;#39;t find single words to describe our thoughts, but we can describe the concepts in phrases. A Reverse Dictionary (RD) is a solution to this problem. A normal word dictionary maps words to their definitions, but a reverse dictionary maps phrases to semantically-similar words.&lt;/p&gt;

&lt;p&gt;This calls for the ability to parse phrases semantically. Now, similarities between words have been studied &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/lnco.362/abstract&quot;&gt;extensively&lt;/a&gt;. These similarities usually rely on a representational space of words, such as &lt;a href=&quot;http://papers.nips.cc/paper/5021-distributed-representations&quot;&gt;word2vec&lt;/a&gt; (constructed by crawling a huge corpus and extracting the contextual similarities between words, which also reflects the semantics). There isn&amp;#39;t such a representational formalism, which encodes phrasal semantics,  available for phrases. Maybe vectors aren&amp;#39;t suitable for phrasal representations anyway, and we should look towards &lt;a href=&quot;http://www.aclweb.org/anthology/D/D10/D10-1115.pdf&quot;&gt;other mathematical structures&lt;/a&gt;, or we should just use sequential parsing while encoding relationships between constituents - as in a &lt;a href=&quot;http://www.aclweb.org/anthology/Q16-1002&quot;&gt;RNN&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The methods just mentioned are the logical steps forward. Once we understand how to represent phrasal semantics, linking it to lexical semantics won&amp;#39;t be hard, thus solving the reverse dictionary problem, hopefully. But there are other &lt;a href=&quot;http://www.oxfordlearnersdictionaries.com/definition/english/jugaad_1?q=jugaad&quot;&gt;jugaad&lt;/a&gt; that could have simple implementations and could already provide excellent performance for a reverse dictionary. The naivest method is to count to number of words in the input phrase which appear in each word&amp;#39;s definition, and use that as a similarity score from the phrase to every word in the lexicon. As you can tell, this method won&amp;#39;t perform very well. &lt;/p&gt;

&lt;p&gt;This approach can be modified to search for lexical relatives of the words in the input phrase and match them with the definitions of the words in the lexicon towards defining a similarity measure. Factors such as frequencies of words (which might distort the similarity measure) appearing in definitions can be taken care of. The word order in the phrase can be factored in by assessing the words&amp;#39; positions in the constitutent tree of the phrase. Negation words, such as &lt;em&gt;not&lt;/em&gt;, can be used as indications to change the constituents following the negation words to their antonyms. All these additions were made by &lt;a href=&quot;http://ieeexplore.ieee.org/document/6060823/&quot;&gt;Shaw, R., et.al.&lt;/a&gt; in their 2013 model, which performed better than the well-known proprietary software, the &lt;a href=&quot;http://www.onelook.com/reverse-dictionary.shtml&quot;&gt;Onelook Reverse Dictionary&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, I am crazy about neural networks. So, this is what I thought - going from the words in the input phrase to the words in whose definitions they were contained, why should one not carry on from those words to the words in whose definitions they lie in, and so on, building a graph out of the entire lexicon? This &lt;em&gt;Reverse Mapping&lt;/em&gt; is the central idea of our approach. It has the intuitive appeal of word meanings converging/composing in a meaningful way onto other words. Of course, for one sensible convergence, there maybe tens of nonsensical covergences. This indeed is a naive approach and we wanted to see how well it could perform (The rationale was - the RD could output tens of words, and all that matters is that the target word be present in there. We could later think of some learning protocol which could selectively enhance connections, something we didn&amp;#39;t get down to).&lt;/p&gt;

&lt;h4&gt;The Approach:&lt;/h4&gt;

&lt;p&gt;As shown in the figure below,&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://www.psych.nyu.edu/pylkkanen/Neural_Bases/13_Function_Words.pdf&quot;&gt;Functional/stop words&lt;/a&gt; are removed from the input phrase. &lt;/li&gt;
&lt;li&gt;The remaining &lt;em&gt;content words&lt;/em&gt; are converted to their base forms using a lemmatiser.&lt;/li&gt;
&lt;li&gt;These &lt;em&gt;input words&lt;/em&gt; are activated on the reverse map constructed using one/many dictionaries.&lt;/li&gt;
&lt;li&gt;The graph is evolved until all words are reached by the signals from each input word

&lt;ul&gt;
&lt;li&gt;To ensure that all words are connected to every other word, we incorporate forward links (word to words contained in its definition) from the words who do not possess sufficient connectivity. This &lt;em&gt;modified&lt;/em&gt; graph&amp;#39;s connectivity is given by the mixed-backlinked-matrix (mBLM), in the paper.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;We thus have a measure of &lt;em&gt;distances&lt;/em&gt; between the input words and words in the lexicon. We then deploy the similarity measure to get the similarity between the input phrase and the words in the lexicon. &lt;/li&gt;
&lt;li&gt;The similarity measure &lt;em&gt;E(W,P)&lt;/em&gt; of a word &lt;em&gt;W&lt;/em&gt; to the input phrase &lt;em&gt;P&lt;/em&gt; is given by, $$\text{E}_{W,P} =\frac{\sum_i \left ( \nu_{P_i}\times d_{W,P_i} \right )^{-1}}{\sum_i \nu_{P_i}^{-1}}$$ where &lt;em&gt;ν&lt;/em&gt; denotes the frequency of appearance in word definitions, and &lt;em&gt;d&lt;/em&gt; the distance on the graph.&lt;/li&gt;
&lt;li&gt;All the words in the lexicon are ranked according to their similarity measures, and outputted.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://novelmartis.github.io/assets/RD_sum.png&quot; alt=&quot;Summary of the Approach&quot;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;As seen in the figure, the input phrase &amp;#39;Son of my parents&amp;#39; does lead to the word &amp;#39;brother&amp;#39; (the target) as a high-ranked candidate output. As can be seen, the approach is pretty naive, and performs a shallow extraction of useful semantics. Let&amp;#39;s see anyway how it compares to the state-of-the-art approaches.&lt;/p&gt;

&lt;h4&gt;Testing procedure:&lt;/h4&gt;

&lt;p&gt;On the lines of &lt;a href=&quot;http://www.aclweb.org/anthology/Q16-1002&quot;&gt;Hill, F., et.al.&lt;/a&gt;, we decided to create a dataset of user-generated phrases given target words. We had to create a new database as we were using a smaller lexicon (3k words) for the first phase of our project. 25 users generated 179 phrases for us. The performance of a reverse dictionary is given by the rank of the target word in its outputs, given an input phrase. We also extracted definitions for those 179 target words from the Macmillian word dictionary - which we did not use in building out graph.&lt;/p&gt;

&lt;h4&gt;Results and Conclusions:&lt;/h4&gt;

&lt;p&gt;For the user-generated phrases, our best model could find the target word in 10% (top-10 outputs), and 53% (top-100 outputs) of the cases, as opposed to 7% (top-10 outputs), and 52% (top-100 outputs) for Onelook. For the Macmillian word definitions, our best model could find the target word in 25% (top-10 outputs), and 84% (top-100 outputs) of the cases, as opposed to 20% (top-10 outputs), and 68% (top-100 outputs) for Onelook. If we were to generate a random selection of words, we could find the target word in 0.01% (top-10 outputs), and 3% (top-100 outputs) of the cases. So, our approach is actually performing well (sanity-check).&lt;/p&gt;

&lt;p&gt;So, our approach performed atleast as well as the &lt;a href=&quot;http://www.onelook.com/reverse-dictionary.shtml&quot;&gt;Onelook Reverse Dictionary&lt;/a&gt; on the 179 user-generated phrases and Macmillian word definitions. We also compared our approach with Onelook on the 200 phrases provided by &lt;a href=&quot;http://www.aclweb.org/anthology/Q16-1002&quot;&gt;Hill, F., et.al.&lt;/a&gt;, and our approach performed atleast as well. Now, the RNN-based approach used by &lt;a href=&quot;http://www.aclweb.org/anthology/Q16-1002&quot;&gt;Hill, F., et.al.&lt;/a&gt; performed only slightly better than Onelook. This implies that the semantics being processed by the RNNs are pretty shallow (as the performance is comparable to our naive approach). &lt;/p&gt;

&lt;p&gt;Our approach doesn&amp;#39;t scale well to a bigger lexicon, as seen in the paper. It nevertheless is a cheap way of converting any dictionary into a reverse dictionary. Our method, and that of &lt;a href=&quot;http://ieeexplore.ieee.org/document/6060823/&quot;&gt;Shaw, R., et.al&lt;/a&gt;, could be treated as baselines in the sense that we know how shallow the semantic processing is. The way forward, of course, are RNNs and other semi/un-supervised architectures, which could extract their own features, and which could settle on novel mathematical structures. The performance baselines provided in our paper could be used as a sanity-check for deep semantic processing in the newer architectures.&lt;/p&gt;

&lt;h4&gt;Acknowledgement:&lt;/h4&gt;

&lt;p&gt;The &amp;quot;we&amp;quot; here - &lt;a href=&quot;https://twitter.com/askvarad&quot;&gt;Varad Choudhari&lt;/a&gt; and me. He took care of all the computational heavylifting. Kudos to him!&lt;/p&gt;

&lt;p&gt;I would like to thank &lt;a href=&quot;https://twitter.com/IonutSorodoc&quot;&gt;Ionut-Teodor Sorodoc&lt;/a&gt;, &lt;a href=&quot;https://www.quora.com/profile/Arpan-Saha&quot;&gt;Arpan Saha&lt;/a&gt;, &lt;a href=&quot;http://www.synapticlee.co.uk/about/&quot;&gt;Julie Lee&lt;/a&gt;, and &lt;a href=&quot;https://www5.unitn.it/People/en/Web/Persona/PER0001015#INFO&quot;&gt;Prof. Roberto Zamparelli&lt;/a&gt; for making useful comments about the project, and the 25 participants who generated the phrases.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Visual infomation processing with biologically-realistic neural networks - Musings</title>
   <link href="http://novelmartis.github.io/2016/07/31/visual-processing-review/"/>
   <updated>2016-07-31T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2016/07/31/visual-processing-review</id>
   <content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Vision for us is so natural, rapid and effortless. But how do we recognise a cat as a cat? We might be identifying multiple features such as its ears, its nose, its shape and size. We might have a template for a cat in our brains, with which we match the input image. Template matching sounds promising but is a procedure that is ridiculously complex. For example, what would the template of a cat contain, and how would the images in the figure below be comparable to that template? &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://novelmartis.github.io/assets/cats_all.png&quot; alt=&quot;Template of a cat?&quot; title=&quot;Template of a cat?&quot;&gt;&lt;/p&gt;

&lt;p&gt;There are two routes to go from these images to the template:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Non-linear 3D spatial transformations and non-linear transformations on color intensities.

&lt;ul&gt;
&lt;li&gt;Distorting the image and changing the color profile to fit the template.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Recognise individual features (ears, eyes, etc.) in those images and build the template using them.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first route can encompass the second, as individual feature recognisers could be treated as non-linear transformations on the pixel map. Handpicked features do not provide optimal performance, making the first route the choice for exploring visual information processing. &lt;/p&gt;

&lt;p&gt;We might be employing a complex template matching approach, but stating that isn&amp;#39;t enough. How do we implement these non-linear transformations? How does a network of neurons implement object recognition? This problem is being solved using the mathematical, algorithmic and neuroscience perspectives. The objective is to understand how a biologically-realistic neural network could implement visual information processing.&lt;/p&gt;

&lt;p&gt;This could be a line-of-attack: &lt;br&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-&quot; data-lang=&quot;&quot;&gt;Visual information processing with biologically-realistic SNNs
├── The problem
├── Previous Work
│   ├── Outline
│   ├── Computer Vision
│   │   └── CNNs, RNNs, etc
│   ├── Biological Modelling
│   │   └── HMAX, etc
│   ├── SNNs
│   └── Additions to the problem
│       └── Unsupervised learning
├── Working with SNNs
│   ├── Architectural constraints
│   │   ├── Operational principles: tight E-I balance, etc.
│   │   └── Neural models - Simplicity vs Accuracy
│   └── Learning Rules
│       ├── ReSuMe, etc
│       ├── Backpropagation modified?
│       └── Other rules? - Explore
└── Further work
    ├── Deciding the learning rules
    └── Generalising to videos, attention, etc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
 </entry>
 
 <entry>
   <title>Current Debates Essay plan</title>
   <link href="http://novelmartis.github.io/2016/07/19/curr-deb-essay-plan/"/>
   <updated>2016-07-19T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2016/07/19/curr-deb-essay-plan</id>
   <content type="html">&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-&quot; data-lang=&quot;&quot;&gt;Visual infomation processing with biologically-realistic SNNs
├── Introduction to the problem
├── Previous Work
│   ├── Outline
│   ├── Computer Vision
│   │   └── CNNs, RNNs, etc
│   ├── Biological Modelling
│   │   └── HMAX, etc
│   ├── SNNs
│   ├── Ruminations
│   └── Additions to the problem
│       └── Unsupervised learning
├── Working with SNNs
│   ├── Outline
│   ├── Architectural constraints
│   │   ├── Operational principles: tight E-I balance, etc.
│   │   └── Neural models - Simplicity vs Accuracy
│   └── Learning Rules
│       ├── ReSuMe, etc
│       ├── Backpropagation modified?
│       └── Other rules? - Explore
└── Concluding Remarks
    ├── Deciding the learning rules
    ├── Generalising to videos, attention, etc
    └── Other comments
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Details:&lt;/h2&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Going from pixels to features, simplest version (full version has motion, overt attention, top-down biases, etc.)&lt;/li&gt;
&lt;li&gt;Turns out to be a hard problem&lt;/li&gt;
&lt;li&gt;Humans do it effortlessly and rapidly

&lt;ul&gt;
&lt;li&gt;We would like to to understand how, so SNN models required&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Previous Work&lt;/h3&gt;

&lt;h4&gt;Outline&lt;/h4&gt;

&lt;h4&gt;Computer Vision&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Early attempts (Clustering, k-means, SVMs)

&lt;ul&gt;
&lt;li&gt;Category grouping requires lots of non-linear transformations&lt;/li&gt;
&lt;li&gt;Locally-correlated information, towards the idea of receptive fields&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Convolutional Neural Networks

&lt;ul&gt;
&lt;li&gt;Incorporating the idea of receptive fields and hierarchical feature-extraction

&lt;ul&gt;
&lt;li&gt;Learnt through backprop&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Exhibit striking results with good features (and &amp;#39;hidden&amp;#39; features)&lt;/li&gt;
&lt;li&gt;Exhibit high correlation of information representation with hVS fMRI data&lt;/li&gt;
&lt;li&gt;Further work (with RNNs to do attention task and multi-object processing - bounding boxes)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Biological Modelling&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Idea of receptive fields in V1, gabor identification, color and shape specificity, face and body areas.&lt;/li&gt;
&lt;li&gt;Putting it all together - HMAX (simple and complex cells with receptive fields)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Spiking Neural Networks&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Biological neuron models

&lt;ul&gt;
&lt;li&gt;HH, then simplifications: Izhi, AEIF, etc.&lt;/li&gt;
&lt;li&gt;Simplifications capture spike times, and adaptation, but not the AP profile, which is a good starting point&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Visual processing with SNNs

&lt;ul&gt;
&lt;li&gt;No idea: EXPLORE, FIND OUT&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Rumination&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Sigmoid neurons and spiking neurons are fundamentally different, but atleast we have guidelines.&lt;/li&gt;
&lt;li&gt;Receptive fields and hierarchies in CNNs are partially motivated by the human visual system, but are they a fundamental computational principle, as suggested by local correlations?&lt;/li&gt;
&lt;li&gt;We need to understand SNNs better to translate CNN-like structure to them.&lt;/li&gt;
&lt;li&gt;Feedback management, operational conditions, learning rules.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Additions to the problem&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Unsupervised feature learning is key - children, during development, do not have access to semantic information. How do they still cluster similar objects together (or do they, without being told so?)

&lt;ul&gt;
&lt;li&gt;Can we train an unsupervised ANN to do object recognition, and then one-shot label learning?&lt;/li&gt;
&lt;li&gt;Can evolutionary algorithms like NEAT work out here - if you initialise a fully connected network, would it acquire the structure of CNNs?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Working with SNNs&lt;/h3&gt;

&lt;h4&gt;Outline&lt;/h4&gt;

&lt;h4&gt;Identifying Architectural Constraints&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Operational Principles

&lt;ul&gt;
&lt;li&gt;tight E-I balance, FIND MORE&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Information Coding principles

&lt;ul&gt;
&lt;li&gt;Rate Code&lt;/li&gt;
&lt;li&gt;Rank Order Code&lt;/li&gt;
&lt;li&gt;Spike-time codes, other codes, FIND OUT&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Learning Rules&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Previous Neural Learning models

&lt;ul&gt;
&lt;li&gt;ReSuMe&lt;/li&gt;
&lt;li&gt;FIND OUT MORE&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Modifying backprop?

&lt;ul&gt;
&lt;li&gt;SpikeProp&lt;/li&gt;
&lt;li&gt;Bengio&amp;#39;s bio-backprop (TO READ), FIND MORE&lt;/li&gt;
&lt;li&gt;The paper where they made the SNN activation function continous for backprop.&lt;/li&gt;
&lt;li&gt;My suggestions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Other rules? FIND OUT&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Concluding Remarks&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Deciding the learning rules&lt;/li&gt;
&lt;li&gt;Building the SNN&lt;/li&gt;
&lt;li&gt;Onwards to unsupervised learning - why is unsupervised learning harder, or is it?&lt;/li&gt;
&lt;li&gt;Generalising to videos, attention, etc (with RNNs, or SNNs)&lt;/li&gt;
&lt;li&gt;Other comments&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Let's get it started!</title>
   <link href="http://novelmartis.github.io/2016/07/12/getting-started/"/>
   <updated>2016-07-12T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2016/07/12/getting-started</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The stage is set. Time to roll..&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Only if life were so. The stage is seldom &lt;em&gt;set&lt;/em&gt;. One should keep rolling though.. or atleast think so. Too philosophical a start!&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll pull in some of my past blog posts, which are scattered all over the place. Then, hopefully, I will keep up the momentum. x&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll eventually add in the project details too.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Arithmetic computations with Spiking Neural Networks</title>
   <link href="http://novelmartis.github.io/2016/06/09/arithmetic-computation/"/>
   <updated>2016-06-09T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2016/06/09/arithmetic-computation</id>
   <content type="html">&lt;p&gt;&lt;em&gt;This one dates back to 2015. It was a part of my Bachelor’s thesis. But this part has applications beyond the objective of the thesis viz. quadcopter control. We published a paper detailing our method. It is available &lt;a href=&quot;http://dx.doi.org/10.1109/IJCNN.2015.7280822&quot;&gt;here&lt;/a&gt; (&lt;a href=&quot;https://www.academia.edu/20315873/Arithmetic_Computing_via_Rate_Coding_in_Neural_Circuits_with_Spike-triggered_Adaptive_Synapses&quot;&gt;open-access&lt;/a&gt;). Note: The description below is a simplistic version of what was done in the paper. All the operations mentioned use well-defined mathematical rules.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spiking_neural_network&quot;&gt;Spiking Neural Networks&lt;/a&gt; are approximations of &lt;a href=&quot;https://en.wikipedia.org/wiki/Biological_neural_network&quot;&gt;biological neural networks&lt;/a&gt;. The approximations range from neuron models to synaptic learning models to network topology. The &lt;a href=&quot;http://eaton.math.rpi.edu/CSUMS/Papers/Neuro/Izhikevich04.pdf&quot;&gt;simplest spiking neuron models&lt;/a&gt; are the LIF, AEIF, and Izhikevich neurons. They try to capture information about the spike-times, approximate the nature of the action potential, and various operational modes (burst, regular, etc.). An extremely realistic model is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model&quot;&gt;Hodgkin-Huxley model&lt;/a&gt;. But it is computationally-intensive, and without access to clusters you cannot simulate a network with hundreds of such neurons. Processes such as &lt;a href=&quot;http://research.mssm.edu/cnic/pdfs/nn0604-567.pdf&quot;&gt;dendritic integration&lt;/a&gt; and AP back-propagation are unaccounted for in these models. But we have to start somewhere, and we usually start simple. AEIF neurons capture many neural firing modes, and other relevant properties such as &lt;a href=&quot;http://www.bio.lmu.de/%7Ebenda/publications/adaptation03/adaptationh.html&quot;&gt;spike-frequency adaptation&lt;/a&gt; and also encode a refractory function. These are the neurons we use in our method.&lt;/p&gt;

&lt;p&gt;So, the problem was this - how can SNNs implement arithmetic operations such as Addition, Subtraction, Multiplication and Division? This turns out to be a non-trivial question. Spiking neurons have non-linear I-O relationships (logarithmic &lt;a href=&quot;ftp://ftp.icsi.berkeley.edu/pub/ai/jagota/vol2_6.pdf&quot;&gt;transfer function&lt;/a&gt;), as shown in the following figure. &lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://novelmartis.github.io/assets/fI_AEIF.png&quot; alt=&quot;f-I plot for AEIF neuron&quot;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;To implement linear operations such as addition or subtraction, we need to linearise the the transfer function. Now, we focussed on the network influences, rather than trying to &lt;a href=&quot;http://arxiv.org/pdf/1410.7881.pdf&quot;&gt;manipulate internal dynamics of neurons&lt;/a&gt;. To that end, we introduced a self-inhibitory loop that pushed the neuron into the higher current domain in the above figure, which is pretty linear. With a linear transfer function at our disposal, we can do addition and subtraction. &lt;/p&gt;

&lt;p&gt;The problem begins with multiplication. I thought about using a recurrent loop with some sort of gated function which would keep adding the same value till we ask to stop it. But then I realised there was a simpler way - use exponentiation and logarithm. &lt;/p&gt;

&lt;p&gt;Now, $$\text{A}\times\text{B}=e^{\text{log(A)} + \text{log(B)}}$$&lt;/p&gt;

&lt;p&gt;So, if we could construct networks with overall exponential and logarithmic transfer functions, we are done! We not only solve multiplication and division, but we can also create any sort of polynomial transfer function by composing appropriate LOG and EXP blocks!&lt;/p&gt;

&lt;p&gt;To implement these LOG and EXP blocks, we turned to adaptive synapses. We designed learning rules that allowed us to generate LOG and EXP responses. Simply put, to generate LOG, the synaptic weight has to decrease with increased pre-synaptic firing rate, and to generate EXP, it has to increase with increasing pre-synaptic firing rate. We then translated these rate based rules into spike-time based rules for performing real-time computations. So, we had EXP and LOG networks, which we composed to implement multiplication and division.&lt;/p&gt;

&lt;p&gt;Yes, we know how to use SNNs to implement addition, subtraction, multiplication, and division now. But there were multiple problems with the approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The synaptic adaptation rule was designed and doesn’t seem to be biologically realistic.&lt;/li&gt;
&lt;li&gt;The time required for the networks to stabilise their multiplicative outputs could extend to 0.5 seconds, which make them impossible to use in quick-response control systems such as the quadcopter core control, or biological systems like the cerebellum.&lt;/li&gt;
&lt;li&gt;The operational spike rates lie between 40-140 Hz, which excludes most of the biological neural rates which are pretty low.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, this is not a biologically realistic model of arithmetic computations, and shouldn’t be treated as such.&lt;/p&gt;

&lt;p&gt;A more realistic method of implementing arithmetic operations needs to take into account the actual operational principles of biological neural networks such as &lt;a href=&quot;http://www.nature.com/neuro/journal/v19/n3/full/nn.4243.html&quot;&gt;tight E-I balance&lt;/a&gt;. There are some &lt;a href=&quot;https://papers.nips.cc/paper/5948-enforcing-balance-allows-local-supervised-learning-in-spiking-recurrent-networks.pdf&quot;&gt;efforts&lt;/a&gt; in this direction. Hopefully, we’ll get to a point where we would be able to converse fluently in neural codes.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Quadcopter control using Spiking Neural Networks</title>
   <link href="http://novelmartis.github.io/2016/06/05/quadcopter-control-using-snn/"/>
   <updated>2016-06-05T00:00:00+02:00</updated>
   <id>http://novelmartis.github.io/2016/06/05/quadcopter-control-using-snn</id>
   <content type="html">&lt;p&gt;&lt;em&gt;This one dates back to 2015. This was the topic of my bachelor’s thesis, which was admittedly left incomplete. Find the report &lt;a href=&quot;https://dx.doi.org/10.6084/m9.figshare.1582657.v1&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spiking_neural_network&quot;&gt;Spiking Neural Networks&lt;/a&gt; hit that sweet spot between biological neural networks and artificial neural networks (although now &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;RNNs&lt;/a&gt; and &lt;a href=&quot;http://arxiv.org/abs/1605.07678&quot;&gt;CNNs&lt;/a&gt; are struggling to take over). To better understand how they function, we thought about using them to solve a well-known complex control problem - balancing and flying a quadcopter. &lt;/p&gt;

&lt;p&gt;The first step was to understand quadcopter flight dynamics. The quadcopter is inherently an unstable system - the smallest uncorrected angular deviation in pitch can send the quadcopter flying out of control. PIDs (compensatory mechanisms that worked by controlling the four rotors) were simple and efficient solutions to this problem. In the first leg of the project, we designed a control algorithm directly taking the dynamical equations of the system into account. Our algorithm outperformed &lt;a href=&quot;https://en.wikipedia.org/wiki/PID_controller&quot;&gt;PID&lt;/a&gt;s, and could recover the quadcopter from a multitude of states (including crazy upside-down states). &lt;/p&gt;

&lt;p&gt;Having understood quadcopter dynamics, we wanted to move ahead to bring in the SNNs. The roadblock was severe. How were we supposed to use SNNs to fly a quadcopter? There were multiple options -&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Learning algorithms such as &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.6325&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;ReSuMe&lt;/a&gt;, &lt;/li&gt;
&lt;li&gt;Evolutionary methods such as &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.5457&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;NEAT&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Hand-assembling a network,&lt;/li&gt;
&lt;li&gt;Understanding a natural control system such as the cerebellum and trying to re-purpose the mechanisms. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(4) is a hard problem, but would definitely be interesting to address someday. We weren’t aware if somebody had used (1), but I wasn’t really impressed by the method. Also, the training would require data from the control system we developed, and we were not sure if the the algorithm would generalise and ‘go beyond’ - which would be the point of using a learning algorithm. (But maybe the ‘i-dont-like-it&amp;#39; factor weighed heavier) (2) was used in a &lt;a href=&quot;http://classes.engr.oregonstate.edu/mime/fall2010/me537/Papers/NN_EA_application_shepherd.pdf&quot;&gt;paper&lt;/a&gt; successfully, but not with SNNs (NEAT with SNNs is a computationally-heavy routine).&lt;/p&gt;

&lt;p&gt;So, were we supposed to hand-assemble the network? That sounded sour. But then my advisor hinted at building plug-and-play spiking network modules which could be assembled to emulate any control algorithm. We were focussed on the basic arithmetic operations - addition, subtraction, multiplication and division - which mostly are sufficient for implementing simple control systems. Also, the control algorithm we designed in the first leg of the project used only polynomial operations. So, we set out on this path, and we came up with a method which lets one implement any polynomial transform on real-time spike trains, and &lt;a href=&quot;http://dx.doi.org/10.1109/IJCNN.2015.7280822&quot;&gt;published a paper&lt;/a&gt; about it. (I will discuss the details in another post)&lt;/p&gt;

&lt;p&gt;It so turned out that the time lags involved in the more complex operations such as multiplication of variables raised to some power, were huge. It became extremely hard to combine the modules we designed to emulate the control algorithm we had built. By that time, my time at IIT Bombay was up, and the project was left unfinished. As tempting as the approach sounded, I now think that it wasn’t the best way to proceed.&lt;/p&gt;

&lt;p&gt;It would be interesting to get around the time lag problem, but I’d put my bets on another approach. In any case, it was an awesome experience trying to bend SNNs to our will - seems we lost.&lt;/p&gt;
</content>
 </entry>
 

</feed>
